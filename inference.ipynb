{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376d6d8f6e66430e8d0ac0df176b5b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.notebook_login()\n",
    "\n",
    "# If got error, refer to https://huggingface.co/docs/huggingface_hub/main/en/guides/cli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"bigcode/starcoderbase-1b\"\n",
    "PERT_MODEL = \"cy948/starcoder-perf-airscript\"\n",
    "device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from Remote source huggingface\n",
    "\n",
    "Model card on https://huggingface.co/cy948/starcoder-perf-airscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cy948/miniconda3/envs/playground/lib/python3.12/site-packages/peft/tuners/adalora/config.py:74: UserWarning: Note that `r` is not used in AdaLora and will be ignored.If you intended to set the initial rank, use `init_r` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "config = PeftConfig.from_pretrained(PERT_MODEL)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n",
    "model = PeftModel.from_pretrained(base_model, PERT_MODEL).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from local\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# load the original model first\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=None,\n",
    "    device_map=None,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").cuda()\n",
    "\n",
    "# merge fine-tuned weights with the base model\n",
    "peft_model_id = f\"model/cy948/{PERT_MODEL}\"\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
    "model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define generation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_completion(prefix, suffix):\n",
    "    text = prompt = f\"\"\"{prefix}{suffix}\"\"\"\n",
    "    model.eval()\n",
    "    outputs = model.generate(\n",
    "        input_ids=tokenizer(text, return_tensors=\"pt\").input_ids.cuda(),\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.3,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.0,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/* 为图表工作表 Chart1 中分类轴设置标题文本 */\n",
      "function setCategoryAxisTitle() {\n",
      "    let chart = Application.Charts.Item(\"Chart1\")\n",
      "    let categoryAxis = chart.Axes.Item(xlCategory)\n",
      "    categoryAxis.Title.Text = \"分类轴标题\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\"\"\n",
    "/* 为图表工作表 Chart1 中分类轴设置标题文本 */\n",
    "\"\"\"\n",
    "\n",
    "print(get_code_completion(prefix, ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill In Middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/*本示例将图表工作表 Chart1 第一个数据系列设置为红色。*/\n",
      "\n",
      " Application.Charts.Item(\"Chart1\").SeriesCollection(1).ColorIndex = 1\n"
     ]
    }
   ],
   "source": [
    "# User need to completion the code in the middle\n",
    "\n",
    "prefix = \"\"\"/*本示例将图表工作表 Chart1 第一个数据系列设置为红色。*/\n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "print(get_code_completion(prefix, suffix))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
